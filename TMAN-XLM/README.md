# TMAN-XLM

## Dependencies

- Python 3
- [NumPy](http://www.numpy.org/)
- [PyTorch](http://pytorch.org/) (currently tested on version 0.4 and 1.0)
- [fastBPE](https://github.com/glample/fastBPE) (generate and apply BPE codes)
- [Moses](http://www.statmt.org/moses/) (scripts to clean and tokenize text only - no installation required)
- [Apex](https://www.github.com/nvidia/apex) (for fp16 training)



# Replacing original data by the merged data

Replace multinli.train.en.tsv by the merged data.

Note: The merged data can be generated by using the code TMAN/TMAN-BERT/create_data.py


# Data pre-processing
```
./get-data-xnli.sh
```


### Train on XNLI from a pretrained model

You can now use the pretrained model for cross-lingual classification. To download a model trained with the command above on the MLM-TLM objective, run:

```
wget -c https://dl.fbaipublicfiles.com/XLM/mlm_tlm_xnli15_1024.pth
```

You can now fine-tune the pretrained model on XNLI, or on one of the English GLUE tasks:

```
python glue-xnli.py
--exp_name test_xnli_mlm_tlm             # experiment name
--dump_path ./dumped/                    # where to store the experiment
--model_path mlm_tlm_xnli15_1024.pth     # model location
--data_path ./data/processed/XLM15       # data location
--transfer_tasks XNLI                    # transfer tasks (XNLI or GLUE tasks)
--optimizer adam,lr=0.000005             # optimizer
--batch_size 8                           # batch size
--n_epochs 3                             # number of epochs
--epoch_size -1                          # number of sentences per epoch
--max_len 128                            # max number of words in sentences
--max_vocab 95000                        # max number of words in vocab
```

